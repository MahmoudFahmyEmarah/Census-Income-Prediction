{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 Testing: Data Preprocessing & Feature Engineering\n",
    "\n",
    "This notebook tests the Phase 2 implementation including:\n",
    "- Preprocessing pipeline\n",
    "- Feature engineering\n",
    "- Missing value handling\n",
    "- Class imbalance handling\n",
    "- Duplicate detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.preprocessing import DataPreprocessor, run_phase2\n",
    "from src.data_pipeline import DataPipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "processed_data = joblib.load(MODELS_DIR / \"processed_data.pkl\")\n",
    "\n",
    "X_train = processed_data['X_train']\n",
    "X_test = processed_data['X_test']\n",
    "y_train = processed_data['y_train']\n",
    "y_test = processed_data['y_test']\n",
    "X_train_balanced = processed_data['X_train_balanced']\n",
    "y_train_balanced = processed_data['y_train_balanced']\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Balanced training data shape: {X_train_balanced.shape}\")\n",
    "print(f\"Original class distribution: {pd.Series(y_train).value_counts().to_dict()}\")\n",
    "print(f\"Balanced class distribution: {pd.Series(y_train_balanced).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessing pipeline\n",
    "pipeline_data = joblib.load(MODELS_DIR / \"preprocessing_pipeline.pkl\")\n",
    "\n",
    "pipeline = pipeline_data['pipeline']\n",
    "label_encoder = pipeline_data['label_encoder']\n",
    "feature_names = pipeline_data['feature_names']\n",
    "preprocessing_stats = pipeline_data['preprocessing_stats']\n",
    "\n",
    "print(\"Pipeline steps:\")\n",
    "for i, (name, step) in enumerate(pipeline.steps):\n",
    "    print(f\"  {i+1}. {name}: {step.__class__.__name__}\")\n",
    "\n",
    "print(f\"\\nTotal features: {len(feature_names)}\")\n",
    "print(f\"Preprocessing stats: {preprocessing_stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Feature Engineering Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check engineered features\n",
    "engineered_features = [\n",
    "    'age_group', 'is_senior', 'is_young_adult',\n",
    "    'education_level', 'has_college_degree', 'has_advanced_degree',\n",
    "    'is_self_employed', 'is_government_worker', 'is_unemployed',\n",
    "    'has_capital_gains', 'has_capital_losses', 'has_dividends',\n",
    "    'is_married', 'is_divorced_separated', 'has_children',\n",
    "    'is_migrant', 'work_intensity', 'is_full_year_worker',\n",
    "    'age_education_interaction', 'married_with_children'\n",
    "]\n",
    "\n",
    "print(\"Engineered features found in dataset:\")\n",
    "found_features = [f for f in engineered_features if any(f in fn for fn in feature_names)]\n",
    "for feature in found_features:\n",
    "    print(f\"  ✓ {feature}\")\n",
    "\n",
    "missing_features = [f for f in engineered_features if not any(f in fn for fn in feature_names)]\n",
    "if missing_features:\n",
    "    print(\"\\nMissing engineered features:\")\n",
    "    for feature in missing_features:\n",
    "        print(f\"  ✗ {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Missing Value Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in processed data\n",
    "missing_train = X_train.isnull().sum().sum()\n",
    "missing_test = X_test.isnull().sum().sum()\n",
    "\n",
    "print(f\"Missing values in processed training data: {missing_train}\")\n",
    "print(f\"Missing values in processed test data: {missing_test}\")\n",
    "\n",
    "# Check for missing indicator features\n",
    "missing_indicators = [col for col in X_train.columns if '_was_missing' in col]\n",
    "print(f\"\\nMissing indicator features created: {len(missing_indicators)}\")\n",
    "for indicator in missing_indicators[:5]:  # Show first 5\n",
    "    print(f\"  - {indicator}\")\n",
    "if len(missing_indicators) > 5:\n",
    "    print(f\"  ... and {len(missing_indicators) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Class Imbalance Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different class imbalance strategies\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Test SMOTE\n",
    "try:\n",
    "    X_smote, y_smote = preprocessor.handle_class_imbalance(X_train.iloc[:1000], y_train[:1000], strategy='smote')\n",
    "    print(f\"SMOTE result: {X_smote.shape}, class distribution: {pd.Series(y_smote).value_counts().to_dict()}\")\n",
    "except Exception as e:\n",
    "    print(f\"SMOTE failed: {e}\")\n",
    "\n",
    "# Test undersampling\n",
    "try:\n",
    "    X_under, y_under = preprocessor.handle_class_imbalance(X_train.iloc[:1000], y_train[:1000], strategy='undersampling')\n",
    "    print(f\"Undersampling result: {X_under.shape}, class distribution: {pd.Series(y_under).value_counts().to_dict()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Undersampling failed: {e}\")\n",
    "\n",
    "# Test balanced sample (used in main pipeline)\n",
    "try:\n",
    "    X_balanced, y_balanced = preprocessor.handle_class_imbalance(X_train.iloc[:1000], y_train[:1000], strategy='balanced_sample')\n",
    "    print(f\"Balanced sample result: {X_balanced.shape}, class distribution: {pd.Series(y_balanced).value_counts().to_dict()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Balanced sample failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data quality\n",
    "print(\"Data Quality Validation:\")\n",
    "print(f\"Training data types: {X_train.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"Test data types: {X_test.dtypes.value_counts().to_dict()}\")\n",
    "\n",
    "# Check for infinite values\n",
    "inf_train = np.isinf(X_train.select_dtypes(include=[np.number])).sum().sum()\n",
    "inf_test = np.isinf(X_test.select_dtypes(include=[np.number])).sum().sum()\n",
    "print(f\"Infinite values in training data: {inf_train}\")\n",
    "print(f\"Infinite values in test data: {inf_test}\")\n",
    "\n",
    "# Check feature consistency between train and test\n",
    "train_cols = set(X_train.columns)\n",
    "test_cols = set(X_test.columns)\n",
    "print(f\"Feature consistency: {train_cols == test_cols}\")\n",
    "if train_cols != test_cols:\n",
    "    print(f\"  Missing in test: {train_cols - test_cols}\")\n",
    "    print(f\"  Extra in test: {test_cols - train_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7: Complete Phase 2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete Phase 2 pipeline\n",
    "results = run_phase2()\n",
    "\n",
    "print(\"Phase 2 Results:\")\n",
    "print(f\"Training shape: {results['train_shape']}\")\n",
    "print(f\"Test shape: {results['test_shape']}\")\n",
    "print(f\"Balanced training shape: {results['balanced_train_shape']}\")\n",
    "print(f\"Feature count: {results['feature_count']}\")\n",
    "print(f\"Phase status: {results['phase_status']}\")\n",
    "\n",
    "# Validate all deliverables\n",
    "deliverables = [\n",
    "    MODELS_DIR / \"processed_data.pkl\",\n",
    "    MODELS_DIR / \"preprocessing_pipeline.pkl\",\n",
    "    REPORTS_DIR / \"preprocessing_report.md\"\n",
    "]\n",
    "\n",
    "print(\"\\nDeliverables Check:\")\n",
    "for deliverable in deliverables:\n",
    "    if deliverable.exists():\n",
    "        print(f\"✓ {deliverable.name}\")\n",
    "    else:\n",
    "        print(f\"✗ {deliverable.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n",
   "name": "python3"\n",
  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.11.0"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}

